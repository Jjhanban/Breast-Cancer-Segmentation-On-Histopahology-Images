{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778b8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from resnet import build_resunet\n",
    "from metricsres import dice_loss, dice_coef, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fca6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1376 - 1376\n",
      "Valid: 458 - 458\n",
      "Test: 458 - 458\n",
      "Epoch 1/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1992 - dice_coef: 0.8008 - iou: 0.6710 - recall: 0.9984 - precision: 0.6721 \n",
      "Epoch 1: val_loss improved from inf to 0.20149, saving model to filesResnet\\modelresunet1.h5\n",
      "115/115 [==============================] - 3933s 34s/step - loss: 0.1992 - dice_coef: 0.8008 - iou: 0.6710 - recall: 0.9984 - precision: 0.6721 - val_loss: 0.2015 - val_dice_coef: 0.7993 - val_iou: 0.6709 - val_recall: 1.0000 - val_precision: 0.6699 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1984 - dice_coef: 0.8016 - iou: 0.6719 - recall: 1.0000 - precision: 0.6720 \n",
      "Epoch 2: val_loss did not improve from 0.20149\n",
      "115/115 [==============================] - 3768s 33s/step - loss: 0.1984 - dice_coef: 0.8016 - iou: 0.6719 - recall: 1.0000 - precision: 0.6720 - val_loss: 0.2015 - val_dice_coef: 0.7993 - val_iou: 0.6709 - val_recall: 1.0000 - val_precision: 0.6699 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1984 - dice_coef: 0.8016 - iou: 0.6719 - recall: 1.0000 - precision: 0.6720 \n",
      "Epoch 3: val_loss did not improve from 0.20149\n",
      "115/115 [==============================] - 3763s 33s/step - loss: 0.1984 - dice_coef: 0.8016 - iou: 0.6719 - recall: 1.0000 - precision: 0.6720 - val_loss: 0.2015 - val_dice_coef: 0.7993 - val_iou: 0.6709 - val_recall: 1.0000 - val_precision: 0.6699 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1984 - dice_coef: 0.8016 - iou: 0.6719 - recall: 1.0000 - precision: 0.6720 \n",
      "Epoch 4: val_loss did not improve from 0.20149\n",
      "115/115 [==============================] - 3709s 32s/step - loss: 0.1984 - dice_coef: 0.8016 - iou: 0.6719 - recall: 1.0000 - precision: 0.6720 - val_loss: 0.2015 - val_dice_coef: 0.7993 - val_iou: 0.6709 - val_recall: 1.0000 - val_precision: 0.6699 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1984 - dice_coef: 0.8016 - iou: 0.6719 - recall: 1.0000 - precision: 0.6720 \n",
      "Epoch 5: val_loss did not improve from 0.20149\n",
      "115/115 [==============================] - 3799s 33s/step - loss: 0.1984 - dice_coef: 0.8016 - iou: 0.6719 - recall: 1.0000 - precision: 0.6720 - val_loss: 0.2015 - val_dice_coef: 0.7993 - val_iou: 0.6709 - val_recall: 1.0000 - val_precision: 0.6699 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Global Parameters \"\"\"\n",
    "H = 256\n",
    "W = 256\n",
    "\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def shuffling(x, y):\n",
    "    x, y = shuffle(x, y, random_state=42)\n",
    "    return x, y\n",
    "\n",
    "def load_data(path, split=0.2):\n",
    "    images = sorted(glob(os.path.join(path, r\"C:\\Users\\User\\Desktop\\cubaan2\\data\\images\", \"*.png\")))\n",
    "    masks = sorted(glob(os.path.join(path, r\"C:\\Users\\User\\Desktop\\cubaan2\\data\\mask\", \"*.png\")))\n",
    "\n",
    "    split_size = int(len(images) * split)\n",
    "\n",
    "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=split_size, random_state=42)\n",
    "\n",
    "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=split_size, random_state=42)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
    "\n",
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    return x\n",
    "\n",
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
    "    x.set_shape([H, W, 3])\n",
    "    y.set_shape([H, W, 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(X, Y, batch_size=4):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(10)\n",
    "    return dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    \"\"\" Directory for storing files \"\"\"\n",
    "    create_dir(\"filesResnet\")\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    batch_size = 12\n",
    "    lr = 0.01\n",
    "    num_epochs = 5\n",
    "    model_path = os.path.join(\"filesResnet\", \"modelresunet1.h5\")\n",
    "    csv_path = os.path.join(\"filesResnet\", \"dataresunet1.csv\")\n",
    "\n",
    "    \"\"\" Dataset \"\"\"\n",
    "    dataset_path = r\"C:\\Users\\User\\Desktop\\cubaan2\\data\"\n",
    "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(dataset_path)\n",
    "    train_x, train_y = shuffling(train_x, train_y)\n",
    "\n",
    "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
    "    print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "    train_dataset = tf_dataset(train_x, train_y, batch_size)\n",
    "    valid_dataset = tf_dataset(valid_x, valid_y, batch_size)\n",
    "\n",
    "    train_steps = len(train_dataset)\n",
    "    valid_steps = len(valid_dataset)\n",
    "\n",
    "    \"\"\" Model \"\"\"\n",
    "    model = build_resunet((H, W, 3))\n",
    "    metrics = [dice_coef, iou, Recall(), Precision()]\n",
    "    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=metrics)\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
    "        CSVLogger(csv_path),\n",
    "        TensorBoard(),\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=valid_dataset,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=valid_steps,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1854c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804153b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypp",
   "language": "python",
   "name": "fypp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
