{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c4ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from vgg19 import build_vgg19_unet\n",
    "from metrics1 import dice_loss, dice_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1ed2f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1376 - 1376\n",
      "Valid: 458 - 458\n",
      "Test: 458 - 458\n",
      "Epoch 1/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0358 - dice_coef: 0.9642 \n",
      "Epoch 1: val_loss improved from inf to 0.19806, saving model to filesV19\\modelvggt1.h5\n",
      "115/115 [==============================] - 3029s 26s/step - loss: 0.0358 - dice_coef: 0.9642 - val_loss: 0.1981 - val_dice_coef: 0.8027 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0212 - dice_coef: 0.9788 \n",
      "Epoch 2: val_loss improved from 0.19806 to 0.18195, saving model to filesV19\\modelvggt1.h5\n",
      "115/115 [==============================] - 3033s 26s/step - loss: 0.0212 - dice_coef: 0.9788 - val_loss: 0.1820 - val_dice_coef: 0.8188 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0188 - dice_coef: 0.9812 \n",
      "Epoch 3: val_loss improved from 0.18195 to 0.01402, saving model to filesV19\\modelvggt1.h5\n",
      "115/115 [==============================] - 3025s 26s/step - loss: 0.0188 - dice_coef: 0.9812 - val_loss: 0.0140 - val_dice_coef: 0.9861 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0113 - dice_coef: 0.9887 \n",
      "Epoch 4: val_loss did not improve from 0.01402\n",
      "115/115 [==============================] - 3007s 26s/step - loss: 0.0113 - dice_coef: 0.9887 - val_loss: 0.0151 - val_dice_coef: 0.9850 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0092 - dice_coef: 0.9908 \n",
      "Epoch 5: val_loss did not improve from 0.01402\n",
      "115/115 [==============================] - 3012s 26s/step - loss: 0.0092 - dice_coef: 0.9908 - val_loss: 0.0248 - val_dice_coef: 0.9754 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Global Parameters \"\"\"\n",
    "H = 256\n",
    "W = 256\n",
    "\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def shuffling(x, y):\n",
    "    x, y = shuffle(x, y, random_state=42)\n",
    "    return x, y\n",
    "\n",
    "def load_data(path, split=0.2):\n",
    "    images = sorted(glob(os.path.join(path, r\"C:\\Users\\User\\Desktop\\cubaan2\\data\\images\", \"*.png\")))\n",
    "    masks = sorted(glob(os.path.join(path, r\"C:\\Users\\User\\Desktop\\cubaan2\\data\\mask\", \"*.png\")))\n",
    "\n",
    "    split_size = int(len(images) * split)\n",
    "\n",
    "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=split_size, random_state=42)\n",
    "\n",
    "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=split_size, random_state=42)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
    "\n",
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    return x\n",
    "\n",
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
    "    x.set_shape([H, W, 3])\n",
    "    y.set_shape([H, W, 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(X, Y, batch_size=4):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(10)\n",
    "    return dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    \"\"\" Directory for storing files \"\"\"\n",
    "    create_dir(\"filesV19\")\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    batch_size = 12\n",
    "    lr = 0.01\n",
    "    num_epochs = 5\n",
    "    model_path = os.path.join(\"filesV19\", \"modelvggt1.h5\")\n",
    "    csv_path = os.path.join(\"filesV19\", \"datavggt1.csv\")\n",
    "\n",
    "    \"\"\" Dataset \"\"\"\n",
    "    dataset_path = r\"C:\\Users\\User\\Desktop\\cubaan2\\data\"\n",
    "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(dataset_path)\n",
    "    train_x, train_y = shuffling(train_x, train_y)\n",
    "\n",
    "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
    "    print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "    train_dataset = tf_dataset(train_x, train_y, batch_size)\n",
    "    valid_dataset = tf_dataset(valid_x, valid_y, batch_size)\n",
    "\n",
    "    train_steps = len(train_dataset)\n",
    "    valid_steps = len(valid_dataset)\n",
    "\n",
    "    \"\"\" Model \"\"\"\n",
    "    model = build_vgg19_unet((H, W, 3))\n",
    "    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef])\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
    "        CSVLogger(csv_path),\n",
    "        TensorBoard(),\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=valid_dataset,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=valid_steps,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ebdf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypp",
   "language": "python",
   "name": "fypp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
