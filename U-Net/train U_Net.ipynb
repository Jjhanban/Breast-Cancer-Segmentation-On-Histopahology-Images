{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efeced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f506f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" #uncesessresy message \n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unet import build_unet\n",
    "from metrics import dice_loss, dice_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14d2230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1376 - 1376\n",
      "Valid: 458 - 458\n",
      "Test : 458 - 458\n",
      "Epoch 1/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0385 - dice_coef: 0.9615 \n",
      "Epoch 1: val_loss improved from inf to 0.19806, saving model to fypjohan\\modell1.h5\n",
      "115/115 [==============================] - 3295s 28s/step - loss: 0.0385 - dice_coef: 0.9615 - val_loss: 0.1981 - val_dice_coef: 0.8026 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0202 - dice_coef: 0.9798 \n",
      "Epoch 2: val_loss improved from 0.19806 to 0.13242, saving model to fypjohan\\modell1.h5\n",
      "115/115 [==============================] - 3160s 27s/step - loss: 0.0202 - dice_coef: 0.9798 - val_loss: 0.1324 - val_dice_coef: 0.8683 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0152 - dice_coef: 0.9848 \n",
      "Epoch 3: val_loss improved from 0.13242 to 0.07374, saving model to fypjohan\\modell1.h5\n",
      "115/115 [==============================] - 3207s 28s/step - loss: 0.0152 - dice_coef: 0.9848 - val_loss: 0.0737 - val_dice_coef: 0.9268 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0090 - dice_coef: 0.9909 \n",
      "Epoch 4: val_loss improved from 0.07374 to 0.01451, saving model to fypjohan\\modell1.h5\n",
      "115/115 [==============================] - 2823s 25s/step - loss: 0.0090 - dice_coef: 0.9909 - val_loss: 0.0145 - val_dice_coef: 0.9856 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0083 - dice_coef: 0.9917 \n",
      "Epoch 5: val_loss did not improve from 0.01451\n",
      "115/115 [==============================] - 2832s 25s/step - loss: 0.0083 - dice_coef: 0.9917 - val_loss: 0.0190 - val_dice_coef: 0.9812 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Global parameters \"\"\"\n",
    "H = 256\n",
    "W = 256\n",
    "\n",
    "def create_dir(path): # make folder and path \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def load_dataset(path, split=0.2):\n",
    "    images = sorted(glob(os.path.join(path, r\"C:\\Users\\User\\Desktop\\cubaan2\\data\\images\", \"*.png\")))\n",
    "    masks = sorted(glob(os.path.join(path, r\"C:\\Users\\User\\Desktop\\cubaan2\\data\\mask\", \"*.png\")))\n",
    "    \n",
    "    split_size = int(len(images) * split)\n",
    "\n",
    "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=split_size, random_state=42)\n",
    "\n",
    "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=split_size, random_state=42)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
    "\n",
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x / 255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (h, w)\n",
    "    x = cv2.resize(x, (W, H))   ## (h, w)\n",
    "    x = x / 255.0               ## (h, w)\n",
    "    x = x.astype(np.float32)    ## (h, w)\n",
    "    x = np.expand_dims(x, axis=-1)## (h, w, 1)\n",
    "    return x\n",
    "\n",
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
    "    x.set_shape([H, W, 3])\n",
    "    y.set_shape([H, W, 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(X, Y, batch=2):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(10)\n",
    "    return dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\" # random \n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    \"\"\" Directory for storing files \"\"\"\n",
    "    create_dir(\"fypjohan\")\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    batch_size = 12 # tukar 8 \n",
    "    lr = 0.01\n",
    "    num_epochs = 5\n",
    "    model_path = os.path.join(\"fypjohan\", \"modell1.h5\")\n",
    "    csv_path = os.path.join(\"fypjohan\", \"logl1.csv\")\n",
    "\n",
    "    \"\"\" Dataset \"\"\"\n",
    "    dataset_path = r\"C:\\Users\\User\\Desktop\\cubaan2\\data\"\n",
    "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path)\n",
    "\n",
    "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
    "    print(f\"Test : {len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "    train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
    "    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n",
    "\n",
    "    \"\"\" Model \"\"\"\n",
    "    model = build_unet((H, W, 3))\n",
    "    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef])\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
    "        CSVLogger(csv_path),\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=valid_dataset,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390da1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1693e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypp",
   "language": "python",
   "name": "fypp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
